{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries loaded.\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../rankfromsets\")\n",
    "from models.models import InnerProduct\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from scipy import sparse\n",
    "import boto3\n",
    "\n",
    "dict_dir = Path(\"../../data/BERT/dictionaries\")\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)\n",
    "print(\"Dictionaries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/rohan/news-classification/data/BERT/dictionaries/reversed_word_ids.json', \"r\") as file:\n",
    "    reversed_word_ids = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries loaded.\n",
      "Data loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndataset.tokenize()\\nproper_data = dataset.map_items(final_word_ids,\\n                    final_url_ids,\\n                    final_publication_ids,\\n                    filter=True,\\n                    min_length=200)\\n\\ndata_path = Path(\"/users/rohan/news-classification/Data/embedding-search/data\")\\nif not data_path.is_dir():\\n    data_path.mkdir()\\nmapped_data_path = data_path / \"mapped-data\"\\nif not mapped_data_path.is_dir():\\n    mapped_data_path.mkdir()\\ntrain_mapped_path = mapped_data_path / \"mapped_dataset.json\"\\nwith open(train_mapped_path, \"w\") as file:\\n    json.dump(proper_data, file)\\nraw_data = Articles(train_mapped_path)\\nprint(\"Final: \", len(raw_data))\\nprint(f\"Filtered, Mapped Data saved to {mapped_data_path} directory\")\\nprint(\"-------------------\")\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path('../../data/BERT/mapped-data/train.json')\n",
    "raw_data = Articles(data_path)\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "dataset.tokenize()\n",
    "proper_data = dataset.map_items(final_word_ids,\n",
    "                    final_url_ids,\n",
    "                    final_publication_ids,\n",
    "                    filter=True,\n",
    "                    min_length=200)\n",
    "\n",
    "data_path = Path(\"/users/rohan/news-classification/Data/embedding-search/data\")\n",
    "if not data_path.is_dir():\n",
    "    data_path.mkdir()\n",
    "mapped_data_path = data_path / \"mapped-data\"\n",
    "if not mapped_data_path.is_dir():\n",
    "    mapped_data_path.mkdir()\n",
    "train_mapped_path = mapped_data_path / \"mapped_dataset.json\"\n",
    "with open(train_mapped_path, \"w\") as file:\n",
    "    json.dump(proper_data, file)\n",
    "raw_data = Articles(train_mapped_path)\n",
    "print(\"Final: \", len(raw_data))\n",
    "print(f\"Filtered, Mapped Data saved to {mapped_data_path} directory\")\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded.\n"
     ]
    }
   ],
   "source": [
    "abs_model_path = Path(\"/users/rohan/news-classification/data/rankfromsets/model/100mean-inner-product-model.pt\")\n",
    "kwargs = dict(n_publications=len(final_publication_ids),\n",
    "              n_articles=len(final_url_ids),\n",
    "              n_attributes=len(final_word_ids),\n",
    "              emb_size=100,\n",
    "              sparse=False,\n",
    "              use_article_emb=False,\n",
    "              mode='mean')\n",
    "model = InnerProduct(**kwargs)\n",
    "model.load_state_dict(torch.load(abs_model_path))\n",
    "print(\"Model Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "cols = []\n",
    "for idx, item in enumerate(raw_data.examples):\n",
    "    word_ids = list(set(item['text']))\n",
    "    number_of_words = np.arange(len(word_ids))\n",
    "    rows.append(np.array(np.ones_like(number_of_words) * idx))\n",
    "    cols.append(np.array(word_ids, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rows = np.concatenate(rows, axis=None)\n",
    "final_cols = np.concatenate(cols, axis=None)\n",
    "final_data = np.ones_like(final_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_articles = csr_matrix((final_data, (final_rows, final_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Created!\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix Created!\")\n",
    "\n",
    "publication_emb = model.publication_embeddings.weight.data[0].cpu().numpy()\n",
    "publication_bias = model.publication_bias.weight.data[0].cpu().numpy()\n",
    "word_emb = model.attribute_emb_sum.weight.data.cpu().numpy()\n",
    "word_bias = model.attribute_bias_sum.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../../data/BERT/u-map/word_embs.tsv\", word_emb, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_pub_emb = np.absolute(publication_emb)\n",
    "abs_indices = np.argsort(abs_pub_emb)\n",
    "ranked_abs_indices = abs_indices.tolist()[::-1][:100]\n",
    "len(ranked_abs_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97058  74350   3422  13576  13336  17505   9678   6039   5244   8132\n",
      " 103482   4337  10204   2768  59956  69910  62884   3486   5542  16999\n",
      "   2667  31863  30074  46135 131804  19594   8315  18405  11226   4612\n",
      "  12533  76975   5295   1064  83067   3398   9118 201038   2666  28494\n",
      " 188036  20795  70641   1274   4016  73256   6429   4924   3400  11513\n",
      "  16931   9608   3346  34346  10397  50922  18877  12284  15644  10817\n",
      "    553  54456  11266   7501  61218  24594  94836   3393   2591  10464\n",
      "  21082    251  21234  15394  17625  25837   3392   2602   4326  40904\n",
      "  34602  61136   4243   3602  28584  18602   8292   8163   2352   1436\n",
      "  13367  19263    903   9119  10078     26  32943   8640  16952    196]\n",
      "[  1065  30817  33846  15394  67162  30163  61218  22415  61135 201038\n",
      "  99425   1948 188036   3392   3251  16999  18877   5542   2666  15644\n",
      "  18405  17832  21121  88959  28494   8063   4160   8315   1469   4016\n",
      "   5295  73204  11226   3422  13576   8512  16931  50922  10817   4924\n",
      "  13717  34346   1064   6039   4096   2885  97058   3346  12155   3398\n",
      "   9118  69910  46135  83067  10204  21234  10464  20795  61136  12284\n",
      "  30074   6429   1436   4243   2667    251 131804    553   9608  11266\n",
      "   8292  94836  34602   7501   3602  24594  25837   2591  28584   2602\n",
      "  73256  18602   9119  13367   4326   2352  21082   3393   8163  17625\n",
      "   3400  19263  40904  10078    903     26  32943  16952   8640    196]\n"
     ]
    }
   ],
   "source": [
    "word_emb_indices = word_emb.argsort(axis=0)\n",
    "print(word_emb_indices[:, 1][-100:])\n",
    "print(word_emb_indices[:, 2][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_word_emb =  word_emb / np.sqrt(np.square(word_emb).sum(axis=-1)).reshape(30522, 1)\n",
    "print(regularized_word_emb.shape)\n",
    "word_emb_indices = regularized_word_emb.argsort(axis=0)\n",
    "i=0\n",
    "for idx in range(100):\n",
    "    print(idx, \": \", publication_emb[idx])\n",
    "    if publication_emb[idx] > 0:\n",
    "        print(\"Positive\")\n",
    "        print(\"---------------------\")\n",
    "        current_column = np.copy(word_emb_indices[:, idx][-200:])\n",
    "        twisted_column = current_column.tolist()[::-1]\n",
    "        top_words = []\n",
    "        for index in twisted_column:\n",
    "            if 'unused' not in reversed_word_ids[str(index)] and len(reversed_word_ids[str(index)]) > 1 and \"##\" not in reversed_word_ids[str(index)]:\n",
    "                print(reversed_word_ids[str(index)], \": \", regularized_word_emb[index, idx])\n",
    "    else:\n",
    "        current_column = np.copy(word_emb_indices[:, idx][:200])\n",
    "        twisted_column = current_column.tolist()\n",
    "        print(\"Negative\")\n",
    "        print(\"---------------------\")\n",
    "        top_words = []\n",
    "        for index in twisted_column:\n",
    "            if 'unused' not in reversed_word_ids[str(index)] and len(reversed_word_ids[str(index)]) > 1 and \"##\" not in reversed_word_ids[str(index)]:\n",
    "                print(reversed_word_ids[str(index)], \": \", regularized_word_emb[index, idx])\n",
    "    print(\"---------------------\")\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_with_bias = word_emb + word_bias\n",
    "word_bias[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_word_emb_indices = emb_with_bias.argsort(axis=0)\n",
    "for idx in range(25):\n",
    "    print(idx, \": \", publication_emb[idx])\n",
    "    if publication_emb[idx] > 0:\n",
    "        print(\"Positive\")\n",
    "        print(\"---------------------\")\n",
    "        current_column = np.copy(normal_word_emb_indices[:, idx][-200:])\n",
    "        twisted_column = current_column.tolist()[::-1]\n",
    "        top_words = []\n",
    "        for index in twisted_column:\n",
    "            if 'unused' not in reversed_word_ids[index] and len(reversed_word_ids[index]) > 1 and \"##\" not in reversed_word_ids[index]: \n",
    "                print(reversed_word_ids[index], \": \", word_emb[index, idx], \"-------\", word_bias[index])\n",
    "    else:\n",
    "        current_column = np.copy(normal_word_emb_indices[:, idx][:200])\n",
    "        twisted_column = current_column.tolist()\n",
    "        print(\"Negative\")\n",
    "        print(\"---------------------\")\n",
    "        top_words = []\n",
    "        for index in twisted_column:\n",
    "            if 'unused' not in reversed_word_ids[index] and len(reversed_word_ids[index]) > 1 and \"##\" not in reversed_word_ids[index]:\n",
    "                print(reversed_word_ids[index], \": \", word_emb[index, idx], \"-------\", word_bias[index])\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = word_articles.dot(word_emb)\n",
    "article_bias = word_articles.dot(word_bias)\n",
    "word_counts = word_articles.sum(axis=1).reshape(word_articles.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(article_embeddings.shape[0], size=10000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_entry_embeddings = article_embeddings[random_indices, :]\n",
    "import pandas as pd\n",
    "chosen_articles = pd.read_json('../../data/BERT/mapped-data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_articles = chosen_articles.iloc[random_indices,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_articles = chosen_articles.drop(columns=['text', 'url', 'model_publication']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../../data/BERT/u-map/article_embs.tsv\", chosen_entry_embeddings, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_articles.to_csv(\"../../data/BERT/u-map/article_heads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chosen_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(47, 100):\n",
    "    altered_emb = np.copy(publication_emb)\n",
    "    altered_emb[index] = 10000\n",
    "    \n",
    "    emb_times_publication = np.dot(article_embeddings, altered_emb.reshape(100,1))\n",
    "\n",
    "    product_with_bias = emb_times_publication + article_bias\n",
    "\n",
    "    final_logits = np.divide(product_with_bias, word_counts) \n",
    "\n",
    "    indices = final_logits.argsort(axis=0)[-75:].reshape(75)\n",
    "    \n",
    "    top_articles = word_articles[indices.tolist()[0]]\n",
    "    \n",
    "    word_logits = np.dot(word_emb, altered_emb.reshape(100,1)) + word_bias\n",
    "\n",
    "    broadcasted_words_per_article = top_articles.toarray() * word_logits.T\n",
    "\n",
    "    sorted_word_indices = broadcasted_words_per_article.argsort(axis=1)\n",
    "\n",
    "    return_articles = []\n",
    "    print(index)\n",
    "    print(\"---------------------\")\n",
    "    return_articles = []\n",
    "    i = 0\n",
    "    for idx in indices.tolist()[0]:\n",
    "        return_articles = []\n",
    "        current_article = raw_data[int(idx)]\n",
    "        current_article['logit'] = float(final_logits[int(idx)])\n",
    "        current_sorted_words = sorted_word_indices[i]\n",
    "        top_words = []\n",
    "        least_words = []\n",
    "        for top_word in current_sorted_words[-10:]:\n",
    "            word = id_to_word[str(top_word)]\n",
    "            top_words.append(word)\n",
    "        for least_word in current_sorted_words[:10]:\n",
    "            word = id_to_word[str(least_word)]\n",
    "            least_words.append(word)\n",
    "        current_article['top_words'] = top_words\n",
    "        current_article['least_words'] = least_words\n",
    "        return_articles.append(current_article)\n",
    "        i += 1\n",
    "        ordered_return_articles = return_articles[::-1]\n",
    "        for article in ordered_return_articles:\n",
    "            print(article['title'])\n",
    "            print(article['link'])\n",
    "            print(article['top_words'])\n",
    "            print(article['least_words'])\n",
    "            print(article['logit'])\n",
    "        print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 13017,  13017,  13017,  32216,  13017,  13017, 110173, 111031,\n",
       "          13017, 100152, 111889,  13017, 111031, 110173,  13017,  32216,\n",
       "          32216, 110173,  13017,  13017,  13017, 111031,  13017, 102570,\n",
       "         111031,  13017,  13017, 109602, 100152, 111806, 100152,  13017,\n",
       "          32216, 100152, 100152, 102570, 110173, 100152,  13017, 111031,\n",
       "          13017,  13017, 111031, 102570, 100152, 111806,  13017, 109602,\n",
       "          13017, 110173, 111031, 110173, 100152,  13017,  13017, 110173,\n",
       "         109602, 102570, 111031,  32216, 109602, 109602, 102570, 106597,\n",
       "         100152,  13017,  13017, 100152, 111031, 109602, 109602, 110173,\n",
       "         109602,  13017,  13017,  13017,  32216, 106597,  13017, 106597,\n",
       "         100152, 110173, 102570, 109602,  13017,  13017,  13017,  13017,\n",
       "          13017,  13017,  13017,  13017,  13017,  13017, 100152, 107710,\n",
       "         100152,  13017,  13017, 111031]], dtype=int64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked = np.argsort(article_embeddings, axis=0)\n",
    "ranked[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(100):\n",
    "    current_iteration = ranked[:, idx][::-1]\n",
    "    listed_iteration = np.squeeze(current_iteration).tolist()[0]\n",
    "    print(idx)\n",
    "    print(\"-----------------------------\")\n",
    "    for top in range(20):   \n",
    "        print(listed_iteration[top], raw_data[listed_iteration[top]]['title'])\n",
    "        print(article_embeddings[listed_iteration[top], idx])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../bert-approach\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from scipy import sparse\n",
    "import boto3\n",
    "\n",
    "dict_dir = Path(\"../../data/BERT/dictionaries\")\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_ids = {v: k for k, v in final_word_ids.items()}\n",
    "with open('/users/rohan/news-classification/data/BERT/dictionaries/reversed_word_ids.json', \"w\") as file:\n",
    "    json.dump(reversed_word_ids, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reversed_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
