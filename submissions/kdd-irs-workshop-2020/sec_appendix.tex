% !TEX root = recommending-interesting-writing.tex

\appendix
\clearpage
\section{Reproducibility supplement: code}
\label{sec:code}
We give an example implementation of \acrlong{rfs} with the inner product
regression function in \Cref{eqn:rankfromsets} in python with the PyTorch
package (version 1.0.0). This shows how easy it is to implement \gls{rfs}; we
used a similar implementation to achieve state-of-the-art results in
\Cref{sec:experiments_arxiv}.
\begin{minted}{python}
import torch
from torch import nn
import data

class InnerProduct(nn.Module):
  def __init__(
        self, n_users, n_items, n_attributes, emb_size):
    super().__init__()
    self.user_embeddings = nn.Embedding(n_users, emb_size)
    self.attribute_emb_sum = nn.EmbeddingBag(
        n_attributes, emb_size, 'mean')
    self.item_embeddings = nn.Embedding(n_items, emb_size)
    self.intercepts = nn.Embedding(n_items, 1)

  def forward(
        self, users, items, item_attributes, offsets):
    user_emb = self.user_embeddings(users)
    attr_emb = self.attribute_emb_mean(
        item_attributes, offsets)
    item_emb = self.item_embeddings(items)
    logits = user_emb * (attr_emb + item_emb)
    logits = logits.sum(-1, keepdim=True)
    return logits + self.intercepts(items)

train_data = data.load_data()
learning_rate, batch_size = 0.1, 1000
model = InnerProduct(train_data.n_users,
                     train_data.n_items,
                     train_data.n_attributes,
                     emb_size=100)
optimizer = torch.optim.SGD(
    model.parameters(), learning_rate)
loss = torch.nn.BCEWithLogitsLoss()
# negative samples are in last half of each batch
labels = torch.cat(torch.ones(batch_size // 2),
                   torch.zeros(batch_size // 2))
for batch in train_data:
  model.zero_grad()
  logits = model(*batch)
  L = loss(logits, labels)
  L.backward()
  optimizer.step()
\end{minted}

\input{table/lstm.tex}

% \section{Reproducibility supplement: permutation-invariant models}
% \label{sec:ctpf-permutation-invariance}

% \paragraph{Matrix factorization permutation-invariant.} We show that
% collaborative topic Poisson factorization~\citep{gopalan2014content-based} is
% permutation-invariant. The likelihood of the attributes (words) associated with
% items (documents) in this model factorizes, and multiplication is
% permutation-invariant. Conditional on the latent item representation $\theta_d$ and
% latent word representation $\beta_v$, every word in the document $w_{dv}$ is
% independent. The joint probability of the words in a document factorizes:
% \begin{align}
%   p(w_{d} \mid \theta_d, \beta_v) &= \prod_{w_{dv} \in w_d} p(w_{dv} \mid \theta_d, \beta_v).
% \end{align}
% In this model, predictions are made using expectations under the posterior. The
% posterior is proportional to the the log joint of the model, and the attributes
% of items (words in documents) enter into the model only via the above product.
% The product of the probability of words in a document is invariant to a
% reordering of the words in the document. Therefore, collaborative topic Poisson
% factorization is permutation-invariant.

\section{Reproducibility supplement: simulation study}
\label{sec:simulation}
% In this simulation, the regression function $f$ was the square kernel between
% the user embedding $\theta_u$ and the mean of item attribute embeddings
% ${\beta_j: j \in x^+_m}$. For a fair comparison, the regression functions in
% \Cref{eqn:rankfromsets,eqn:neural-network,eqn:residual} were chosen to have the
% same number of parameters. For data generated using the square kernel, the
% residual and deep parameterizations outperformed the inner product
% parameterization.

With finite data and a finite number of paramaters, the optimal parameterization
of \gls{rfs} is dependent on the data-generating distribution. Recall that
observations of user-item interactions are generated by a Bernoulli distribution
with logit function $f$. We describe a choice of logit function $f$ that leads
to the residual and deep architectures in \Cref{eqn:residual,eqn:neural-network}
outperforming the inner product architecture in \Cref{eqn:rankfromsets} in terms
of predictive performance. Note that the number of parameters across
architectures must be equal for a fair comparison. The code required to
replicate this experiment is included here:

%\url{https://github.com/kdd-anonymous/rankfromsets}.

We simulate data from the following generative process:

\begin{enumerate}
  \item {\bf For every user $u$:}
    \begin{enumerate}
    \item Draw user embedding $\theta_u \sim \textrm{Normal}(0, \mbI)$.
    \end{enumerate}
  \item {\bf For every item attribute $j$:}
    \begin{enumerate}
    \item Draw attribute embedding $\beta_j \sim \textrm{Normal}(0, \mbI)$.
    \end{enumerate}
  \item {\bf For every item $m$:}
    \begin{enumerate}
    \item Draw item topics $\theta_m \sim \textrm{Dirichlet}(\alpha)$
    \item Draw number of item attributes $M \sim \textrm{Poisson}(\lambda)$
    \item Draw nonzero item attributes $x_m \sim \textrm{Multinomial}(M, \theta_m)$.
    \end{enumerate}
  \item {\bf For every user, item observation:}
    \begin{enumerate}
    \item
      $\yum \sim \textrm{Bernoulli}\left(\yum; \sigma(f(\theta_u, x_m)\right)$
    \end{enumerate}
\end{enumerate}

The logit function $f$ is the square kernel:

$$f(\theta_u, x_m) = \left(\theta_u^\top \frac{1}{|x_m|}\sum_{j \in x_m}
  \beta_j\right)^2.$$

Before sampling from the Bernoulli, we standardize the logits output by $f$
across users and subtract $7$ to achieve sparse user-item observations.

\paragraph{Simulation setup} We set the Dirichlet parameter to be $\alpha=0.01$
and the Poisson rate to be $\lambda=20$. We generate data for $1$k users, $5$k
item attributes, $30$k items, and hold out $100$ users for each of the
validation and test sets. We fix the momentum to
$0.9$~\cite{sutskever2013on-the-importance} and grid search over stochastic
gradient descent learning rates of ${10, 1, 0.1, 0.01}$ and over two learning
rate decay schedules. The first linear learning rate decay goes to zero over
$100$k iterations, while the second divides the learning rate by $10$ if the
validation in-matrix recall does not improve (evaluation is performed every
$500$ iterations). We run the grid search on one instance of data generated from
this model, then for the best performing hyperparameters for each model trained
on this instance, we regenerate data $30$ times and average results over these
synthetic datasets.

\input{table/simulation}

\paragraph{Simulation results} The results in \Cref{tab:simulation} demonstrate
that the residual model outperforms both the deep and inner product
architectures for data generated by the above generative process.

\paragraph{Generalization} The above example shows that the choice of architecture in
\gls{rfs} is data-dependent. To ensure that the model does not
overfit as new users or items are included in the training data, we need to
compare the number of parameters to the number of datapoints. A model with
parameters the size of the training data can overfit by memorizing the training
data. For generalization to be possible, overfitting can be avoided if the
number of parameters grows slower than the size of the data. The technical
backing for this comes from asymptotic statistics and the concept of sieved
likelihoods. Specifically, the maximum likelihood estimation procedure with the
objective function in \Cref{eq:objective} can be replaced by maximization of a
sieved likelihood function. The `sieve' refers to filtering information as the
number of parameters (in this case, user and item representations) grows with
the number of observations. The sieved likelihood function enables the analysis
of asymptotic behavior as the number of users grows $U\rightarrow \infty$ and
the number of items grows $I\rightarrow \infty$, An example of a technique to
grow the number of parameters in a way that supports generalization is given in
Chapter 25 of \citet{vaart1998asymptotic}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "set_recommendation"
%%% End: